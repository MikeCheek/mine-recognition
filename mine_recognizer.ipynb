{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mine Recognition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms, models\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Load and Prepare the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the JSON file\n",
    "with open('images/data.json', 'r') as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "# Convert to DataFrame\n",
    "df = pd.DataFrame(data)\n",
    "df['image_path'] = df['file_name']  # Ensure file names match your actual path\n",
    "\n",
    "# Split data into train and validation\n",
    "train_df, val_df = train_test_split(df, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Create a Custom Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MineDataset(Dataset):\n",
    "    def __init__(self, img_dir, transform=None):\n",
    "        self.img_dir = img_dir\n",
    "        self.transform = transform\n",
    "        self.image_paths = []\n",
    "        self.labels = []\n",
    "\n",
    "        # Load image paths and labels based on folder structure\n",
    "        for label, class_name in enumerate(['false', 'true']):\n",
    "            class_dir = os.path.join(img_dir, class_name)\n",
    "            for img_file in os.listdir(class_dir):\n",
    "                self.image_paths.append(os.path.join(class_dir, img_file))\n",
    "                self.labels.append(label)  # 0 for 'false', 1 for 'true'\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = self.image_paths[idx]\n",
    "        image = Image.open(img_path).convert('RGB')\n",
    "        label = self.labels[idx]\n",
    "\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        return image, label\n",
    "\n",
    "# Define image transformations with augmentation\n",
    "transform = transforms.Compose([\n",
    "    transforms.RandomHorizontalFlip(),  # Randomly flip the image horizontally\n",
    "    transforms.RandomRotation(10),      # Randomly rotate the image by Â±10 degrees\n",
    "    transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),  # Randomly change brightness, contrast, saturation, and hue\n",
    "    transforms.Resize((224, 224)),      # Resize to the target size\n",
    "    transforms.ToTensor(),               # Convert the image to a tensor\n",
    "    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])  # Normalize with ImageNet statistics\n",
    "])\n",
    "\n",
    "# Initialize datasets and loaders\n",
    "img_dir = 'images'  # Parent directory containing 'true' and 'false' subdirectories\n",
    "dataset = MineDataset(img_dir=img_dir, transform=transform)\n",
    "\n",
    "# Optionally split into train and validation sets\n",
    "train_size = int(0.8 * len(dataset))\n",
    "val_size = len(dataset) - train_size\n",
    "train_dataset, val_dataset = torch.utils.data.random_split(dataset, [train_size, val_size])\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Define the Model\n",
    "Load a pre-trained model, such as ResNet18, and modify it for binary classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MineClassifier(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MineClassifier, self).__init__()\n",
    "        \n",
    "        # Convolutional layers\n",
    "        self.conv1 = nn.Conv2d(in_channels=3, out_channels=32, kernel_size=3, stride=1, padding=1)\n",
    "        self.conv2 = nn.Conv2d(in_channels=32, out_channels=64, kernel_size=3, stride=1, padding=1)\n",
    "        self.conv3 = nn.Conv2d(in_channels=64, out_channels=128, kernel_size=3, stride=1, padding=1)\n",
    "        \n",
    "        # Pooling layer\n",
    "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2, padding=0)\n",
    "        \n",
    "        # Fully connected layers\n",
    "        self.fc1 = nn.Linear(128 * 28 * 28, 256)  # Adjust dimensions based on input size\n",
    "        self.fc2 = nn.Linear(256, 1)\n",
    "        \n",
    "        # Activation function and dropout\n",
    "        self.relu = nn.ReLU()\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Convolutional layers with pooling and ReLU activation\n",
    "        x = self.pool(self.relu(self.conv1(x)))\n",
    "        x = self.pool(self.relu(self.conv2(x)))\n",
    "        x = self.pool(self.relu(self.conv3(x)))\n",
    "        \n",
    "        # Flatten the tensor for the fully connected layer\n",
    "        x = x.view(-1, 128 * 28 * 28)  # Adjust this based on input dimensions\n",
    "        \n",
    "        # Fully connected layers with dropout\n",
    "        x = self.relu(self.fc1(x))\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc2(x)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EnhancedMineClassifier(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(EnhancedMineClassifier, self).__init__()\n",
    "        \n",
    "        # Convolutional layers with Batch Normalization\n",
    "        self.conv1 = nn.Conv2d(in_channels=3, out_channels=32, kernel_size=3, padding=1)\n",
    "        self.bn1 = nn.BatchNorm2d(32)\n",
    "        \n",
    "        self.conv2 = nn.Conv2d(in_channels=32, out_channels=64, kernel_size=3, padding=1)\n",
    "        self.bn2 = nn.BatchNorm2d(64)\n",
    "        \n",
    "        self.conv3 = nn.Conv2d(in_channels=64, out_channels=128, kernel_size=3, padding=1)\n",
    "        self.bn3 = nn.BatchNorm2d(128)\n",
    "        \n",
    "        self.conv4 = nn.Conv2d(in_channels=128, out_channels=256, kernel_size=3, padding=1)\n",
    "        self.bn4 = nn.BatchNorm2d(256)\n",
    "\n",
    "        self.conv5 = nn.Conv2d(in_channels=256, out_channels=512, kernel_size=3, padding=1)\n",
    "        self.bn5 = nn.BatchNorm2d(512)\n",
    "        \n",
    "        # Pooling layer\n",
    "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "\n",
    "        # Fully connected layers\n",
    "        self.fc1 = nn.Linear(512 * 7 * 7, 1024)  # Adjust dimensions based on input size\n",
    "        self.fc2 = nn.Linear(1024, 256)\n",
    "        self.fc3 = nn.Linear(256, 1)\n",
    "        \n",
    "        # Activation function and dropout\n",
    "        self.relu = nn.ReLU()\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Convolutional layers with ReLU, BatchNorm, and Pooling\n",
    "        x = self.pool(self.relu(self.bn1(self.conv1(x))))\n",
    "        x = self.pool(self.relu(self.bn2(self.conv2(x))))\n",
    "        x = self.pool(self.relu(self.bn3(self.conv3(x))))\n",
    "        x = self.pool(self.relu(self.bn4(self.conv4(x))))\n",
    "        x = self.pool(self.relu(self.bn5(self.conv5(x))))\n",
    "        \n",
    "        # Flatten the tensor for the fully connected layer\n",
    "        x = x.view(-1, 512 * 7 * 7)  # Adjust this based on your input dimensions\n",
    "        \n",
    "        # Fully connected layers with dropout\n",
    "        x = self.relu(self.fc1(x))\n",
    "        x = self.dropout(x)\n",
    "        x = self.relu(self.fc2(x))\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc3(x)\n",
    "        \n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Set Loss Function and Optimizer\n",
    "Binary cross-entropy loss with logits and Adam optimizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Initialize the custom model\n",
    "\n",
    "model = MineClassifier()\n",
    "# model = EnhancedMineClassifier()\n",
    "\n",
    "# Move model to GPU if available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = model.to(device)\n",
    "\n",
    "# Define loss and optimizer\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Training Loop\n",
    "Train the model and evaluate on the validation set after each epoch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/10], Loss: 0.6901, Val Loss: 0.1022, Accuracy: 1.0000, Precision: 1.0000, Recall: 1.0000, F1 Score: 1.0000\n",
      "Epoch [2/10], Loss: 1.1643, Val Loss: 5.7736, Accuracy: 0.0000, Precision: 1.0000, Recall: 0.0000, F1 Score: 0.0000\n",
      "Epoch [3/10], Loss: 2.9097, Val Loss: 0.7926, Accuracy: 0.5000, Precision: 1.0000, Recall: 0.5000, F1 Score: 0.6667\n",
      "Epoch [4/10], Loss: 0.5627, Val Loss: 0.1671, Accuracy: 1.0000, Precision: 1.0000, Recall: 1.0000, F1 Score: 1.0000\n",
      "Epoch [5/10], Loss: 0.6486, Val Loss: 0.1724, Accuracy: 1.0000, Precision: 1.0000, Recall: 1.0000, F1 Score: 1.0000\n",
      "Epoch [6/10], Loss: 0.4412, Val Loss: 0.2318, Accuracy: 1.0000, Precision: 1.0000, Recall: 1.0000, F1 Score: 1.0000\n",
      "Epoch [7/10], Loss: 0.3593, Val Loss: 0.4627, Accuracy: 0.7500, Precision: 1.0000, Recall: 0.7500, F1 Score: 0.8571\n",
      "Epoch [8/10], Loss: 0.3038, Val Loss: 0.1574, Accuracy: 1.0000, Precision: 1.0000, Recall: 1.0000, F1 Score: 1.0000\n",
      "Epoch [9/10], Loss: 0.2216, Val Loss: 0.3299, Accuracy: 0.7500, Precision: 1.0000, Recall: 0.7500, F1 Score: 0.8571\n",
      "Epoch [10/10], Loss: 0.1469, Val Loss: 0.0764, Accuracy: 1.0000, Precision: 1.0000, Recall: 1.0000, F1 Score: 1.0000\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 10\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    \n",
    "    # Training phase\n",
    "    for images, labels in train_loader:\n",
    "        images, labels = images.to(device), labels.to(device).float()\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images).squeeze(1)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        running_loss += loss.item() * images.size(0)\n",
    "    \n",
    "    # Validation phase\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    all_labels = []\n",
    "    all_preds = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for images, labels in val_loader:\n",
    "            images, labels = images.to(device), labels.to(device).float()\n",
    "            outputs = model(images).squeeze(1)\n",
    "            \n",
    "            # Calculate loss\n",
    "            loss = criterion(outputs, labels)\n",
    "            val_loss += loss.item() * images.size(0)\n",
    "            \n",
    "            # Store predictions and true labels\n",
    "            preds = torch.sigmoid(outputs) > 0.5  # Apply threshold to get binary predictions\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "            all_preds.extend(preds.cpu().numpy())\n",
    "    \n",
    "    # Calculate metrics\n",
    "    accuracy = accuracy_score(all_labels, all_preds)\n",
    "    precision = precision_score(all_labels, all_preds, zero_division=1)\n",
    "    recall = recall_score(all_labels, all_preds)\n",
    "    f1 = f1_score(all_labels, all_preds)\n",
    "    \n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {running_loss/len(train_loader.dataset):.4f}, \"\n",
    "          f\"Val Loss: {val_loss/len(val_loader.dataset):.4f}, \"\n",
    "          f\"Accuracy: {accuracy:.4f}, Precision: {precision:.4f}, Recall: {recall:.4f}, F1 Score: {f1:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 7: Predict on New Images\n",
    "Now, we can classify new images that don't have labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8.7087%\n",
      "Prediction: No Mine\n"
     ]
    }
   ],
   "source": [
    "def classify_image(model, image_path):\n",
    "    model.eval()\n",
    "    image = Image.open(image_path).convert('RGB')\n",
    "    image = transform(image).unsqueeze(0).to(device)  # Add batch dimension and move to device\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        output = model(image)\n",
    "        prediction = torch.sigmoid(output).item()  # Sigmoid to get probability\n",
    "        print(f\"{prediction*100:.4f}%\")\n",
    "        return 1 if prediction > 0.5 else 0  # Threshold of 0.5 for binary classification\n",
    "\n",
    "# Example of classifying a new image\n",
    "new_image_path = 'images/unknown0.jpg'\n",
    "prediction = classify_image(model, new_image_path)\n",
    "print(f\"Prediction: {'Mine' if prediction == 1 else 'No Mine'}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mine-recognition",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
